{"cells":[{"metadata":{},"cell_type":"markdown","source":"InstaDeep ML test <br>\nAuthor: Mously Diaw\n\n# Protein sequence classification\n\nProteins are sequence of amino acids, which can be encoded by a single letter. This project aims at building a deep learning model that assigns amino acid sequences to their protein family. The [Pfam dataset](https://www.kaggle.com/googleai/pfam-seed-random-split) is used for this task. \n\n1. [Dataset analysis](#section-one) <br>\n2. [Classifier](#section-two) <br>\n3. [Conclusion](#section-three) <br>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Import packages\n%matplotlib inline\nimport os\nimport numpy as np  \nimport pandas as pd \nimport matplotlib.pyplot as plt\n#import re \nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical \nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder \nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Embedding, LSTM, Conv1D, MaxPooling1D, Bidirectional","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# 1. Dataset analysis "},{"metadata":{},"cell_type":"markdown","source":"## 1.1. Loading the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(partition,col_list):\n    data = []\n    for fn in os.listdir(os.path.join(data_path, partition)):\n        with open(os.path.join(data_path, partition, fn)) as f:\n            data.append(pd.read_csv(f, index_col=None,usecols=col_list))\n    return pd.concat(data) ","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We wish to predict *family_accession* given *sequence* so we will focus on these two attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = '../input/pfam-seed-random-split/random_split/random_split/'\ncol_list = [\"family_accession\",\"sequence\"]\ndf_train = load_data('train',col_list)\ndf_val = load_data('dev',col_list)\ndf_test = load_data('test',col_list)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Data size:')\nprint('Train size :', len(df_train))\nprint('Val size :', len(df_val))\nprint('Test size :', len(df_test))","execution_count":4,"outputs":[{"output_type":"stream","text":"Data size:\nTrain size : 1086741\nVal size : 126171\nTest size : 126171\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"  family_accession                                           sequence\n0       PF08891.11  IEFEEKMLELIDARIESASDDELFAGGYLRGHISLSVANCEEQGIN...\n1       PF01033.17           TTCAVRGCSDINLNQNCQCDPSCYSFGDCCADFAAVCESS\n2       PF01805.20  RNIIDKLAQFVARNGPEFEQMTKNKQKDNPKFSFLFGGEYFNYYQYKVT\n3       PF06799.11  AVPREQRPVNELQQLKDTPLLAWATLDLPQYAQRLLILYGGVFLLL...\n4        PF15494.6  GPNFILEVYSPVSQTWYPVCQDDWTDDFGKIACEDMGYNVDTYYYS...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>family_accession</th>\n      <th>sequence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PF08891.11</td>\n      <td>IEFEEKMLELIDARIESASDDELFAGGYLRGHISLSVANCEEQGIN...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PF01033.17</td>\n      <td>TTCAVRGCSDINLNQNCQCDPSCYSFGDCCADFAAVCESS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PF01805.20</td>\n      <td>RNIIDKLAQFVARNGPEFEQMTKNKQKDNPKFSFLFGGEYFNYYQYKVT</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PF06799.11</td>\n      <td>AVPREQRPVNELQQLKDTPLLAWATLDLPQYAQRLLILYGGVFLLL...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PF15494.6</td>\n      <td>GPNFILEVYSPVSQTWYPVCQDDWTDDFGKIACEDMGYNVDTYYYS...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 1.2. Exploratory data analysis (EDA) and data pre-processing"},{"metadata":{},"cell_type":"markdown","source":"### 1.2.1. Preliminary processing and EDA"},{"metadata":{},"cell_type":"markdown","source":"Let's first merge the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = [df_train, df_val, df_test]\ndf_full = pd.concat(frames)","execution_count":6,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Total number of unique protein families:',len(df_full.family_accession.unique()))","execution_count":7,"outputs":[{"output_type":"stream","text":"Total number of unique protein families: 17929\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Since the data set has over a million samples, we will first work with a sample of it.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"n_samples = 12000\ndf_partial = df_full.sample(n = n_samples, random_state=1)\nnum_classes = len(df_partial.family_accession.unique())\nprint('Number of unique protein families in the small training set:',num_classes)","execution_count":8,"outputs":[{"output_type":"stream","text":"Number of unique protein families in the small training set: 5770\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We will now compute the number of occurences of each amino acid and the length of the sequences in the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count frequency amino acids\nkeys = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L','M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y','X', 'U', 'B', 'O', 'Z']\n\nd = {key: 0 for key in keys}\nfor sequence in df_partial.sequence:     \n    for char in sequence: \n        d[char] = d[char] + 1 \n\n# Order by frequency \nd_sorted = sorted(((d[key],key) for key in d), reverse =True) ","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_freq = pd.DataFrame(d_sorted, columns=['frequency', 'Amino acid'])\ndf_freq.plot(kind='bar', x='Amino acid')\nplt.title(\"Frequency of the amino acids (training set)\")\nplt.show() ","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 20 common amino acids. X,U,Z,B,O are the least frequent. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_length = [len(sequence) for sequence in df_partial.sequence]\nplt.hist(sequence_length)\nplt.xlabel('Sequence length')\nplt.ylabel('Frequency')\nplt.title('Distribution of sequence length (training set)') \nplt.xlim(0, 1250) \nplt.show()","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"The figure above shows that most of the sequences have less than 200 amino acids.  "},{"metadata":{},"cell_type":"markdown","source":"### 1.2.2. Sequence processing "},{"metadata":{},"cell_type":"markdown","source":"Before feeding the data to a neural network, the sequences must be converted into tensors. We chose to tokenize the sequences by breaking them into characters. We then vectorize them (convert each character to a number) using a dictionary with the 20 most common amino acids; the 5 least frequent ones are affected the value '21'. After vectorization, we bring all sequences to the same length before one-hot encoding them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"amino_acids = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L','M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n\ndef create_dict(codes):\n    char_dict = {}\n    for index, val in enumerate(codes):\n        char_dict[val] = index+1\n    return char_dict\n\nchar_dict = create_dict(amino_acids)\n\nprint(\"Dictionary:\", char_dict)","execution_count":12,"outputs":[{"output_type":"stream","text":"Dictionary: {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def vectorized_sequence(sequence):\n    \"\"\"\n    Encodes the sequence using predefined dictionary. The 5 least frequent amino acids are affected the value '21'\n    \"\"\"\n    code = []\n    l = [char for char in sequence]\n    for char in l:\n        code.append(char_dict.get(char, 21))    \n    return code   ","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's illustrate the process with the following sequence"},{"metadata":{"trusted":true},"cell_type":"code","source":"idx=90\nprint(\"Amino acid sequence:\",df_partial.sequence.iloc[idx])","execution_count":14,"outputs":[{"output_type":"stream","text":"Amino acid sequence: SLCNQLQCAAGKWFHEHSKDNGGSNKKTWCEFWEKEGVKPKLQDLFEKIKSGGQDTNDVCNKFGDENPDSVERKACNHITAGLQHIKDIPPSGSGNGVVQSKDQDNQLLQRAVGCIALNMYADQIIAKSKDSCPIDESKIQDMFTKWNEKYNNNSSSSPSCNDVNNKDCFVCKRVQKSELNNCQL\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorized_sequences = [vectorized_sequence(sequence) for sequence in df_partial.sequence]  ","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Vectorized sequence:\", vectorized_sequences[idx])","execution_count":16,"outputs":[{"output_type":"stream","text":"Vectorized sequence: [16, 10, 2, 12, 14, 10, 14, 2, 1, 1, 6, 9, 19, 5, 7, 4, 7, 16, 9, 3, 12, 6, 6, 16, 12, 9, 9, 17, 19, 2, 4, 5, 19, 4, 9, 4, 6, 18, 9, 13, 9, 10, 14, 3, 10, 5, 4, 9, 8, 9, 16, 6, 6, 14, 3, 17, 12, 3, 18, 2, 12, 9, 5, 6, 3, 4, 12, 13, 3, 16, 18, 4, 15, 9, 1, 2, 12, 7, 8, 17, 1, 6, 10, 14, 7, 8, 9, 3, 8, 13, 13, 16, 6, 16, 6, 12, 6, 18, 18, 14, 16, 9, 3, 14, 3, 12, 14, 10, 10, 14, 15, 1, 18, 6, 2, 8, 1, 10, 12, 11, 20, 1, 3, 14, 8, 8, 1, 9, 16, 9, 3, 16, 2, 13, 8, 3, 4, 16, 9, 8, 14, 3, 11, 5, 17, 9, 19, 12, 4, 9, 20, 12, 12, 12, 16, 16, 16, 16, 13, 16, 2, 12, 3, 18, 12, 12, 9, 3, 2, 5, 18, 2, 9, 15, 18, 14, 9, 16, 4, 10, 12, 12, 2, 14, 10]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 200\npadded_sequences = pad_sequences(vectorized_sequences , maxlen=maxlen, padding='post')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Padded sequence:\", padded_sequences[idx])","execution_count":18,"outputs":[{"output_type":"stream","text":"Padded sequence: [16 10  2 12 14 10 14  2  1  1  6  9 19  5  7  4  7 16  9  3 12  6  6 16\n 12  9  9 17 19  2  4  5 19  4  9  4  6 18  9 13  9 10 14  3 10  5  4  9\n  8  9 16  6  6 14  3 17 12  3 18  2 12  9  5  6  3  4 12 13  3 16 18  4\n 15  9  1  2 12  7  8 17  1  6 10 14  7  8  9  3  8 13 13 16  6 16  6 12\n  6 18 18 14 16  9  3 14  3 12 14 10 10 14 15  1 18  6  2  8  1 10 12 11\n 20  1  3 14  8  8  1  9 16  9  3 16  2 13  8  3  4 16  9  8 14  3 11  5\n 17  9 19 12  4  9 20 12 12 12 16 16 16 16 13 16  2 12  3 18 12 12  9  3\n  2  5 18  2  9 15 18 14  9 16  4 10 12 12  2 14 10  0  0  0  0  0  0  0\n  0  0  0  0  0  0  0  0]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"oh_sequences = to_categorical(padded_sequences)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq = oh_sequences[idx]\nplt.matshow(seq.transpose())\nplt.title(\"Padded sequence of shape %s\" % (seq.shape,) + \"\\n\")\nplt.show()","execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1152x144 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"Finally we will transform the labels by integer encoding. "},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df_partial.family_accession) ","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will split the sampled data into the training, validation and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = padded_sequences\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Length train set:\",len(X_train))\nprint(\"Length val set:\",len(X_val))\nprint(\"Length test set:\",len(X_test))","execution_count":23,"outputs":[{"output_type":"stream","text":"Length train set: 9720\nLength val set: 1080\nLength test set: 1200\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n# 2. Classifier "},{"metadata":{},"cell_type":"markdown","source":"Let's first build a baseline model."},{"metadata":{},"cell_type":"markdown","source":"To train the model, we will load the data in batches by creating a data iterator. "},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataIterator:\n    \"\"\" Collects data and yields bunch of batches of data\n    Takes data sources and batch_size as arguments \"\"\"\n    def _init_(self, data1,data2, batch_size,iters):\n        self.data1 = data1\n        self.data2 = data2\n        self.batch_size = batch_size\n        self.iter = iters\n        \n    def next_batch(self):\n        try:\n            idxs = next(self.iter)\n        except StopIteration:\n            self.iter = self.make_random_iter()\n            idxs = next(self.iter)\n        X =[self.data1[i] for i in idxs]\n        Y =[self.data2[i] for i in idxs]\n        X = np.array(X)\n        Y = np.array(Y)\n        return X, Y\n    \n    def make_random_iter(self):\n        splits = np.arange(self.batch_size, len(self.data1),self.batch_size)\n        it = np.split(np.random.permutation(range(len(self.data1))), splits)[:-1]\n        return iter(it)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_val_model(model,epochs,X_train,y_train,batch_size,X_val,y_val):\n    train_iter = DataIterator() \n    train_iter.data1 = X_train\n    train_iter.data2 = y_train\n    train_iter.batch_size = batch_size\n    train_iter.iter = train_iter.make_random_iter()\n    training_iters = np.ceil(len(y_train)/train_iter.batch_size)\n    acc_epochs =[]\n    loss_epochs = []\n    val_acc_epochs =[]\n    val_loss_epochs = []\n    for epoch in range(epochs):  \n        print(\"\\n\" + \"Epoch:\",epoch)\n        step = 0\n        acc = 0\n        loss = 0\n        val_acc = 0\n        val_loss = 0\n        while  step  <= training_iters:\n            batch_x, batch_y = train_iter.next_batch()\n            history = model.fit(batch_x, batch_y , epochs=1,batch_size= train_iter.batch_size, \n                            validation_data=(X_val, y_val),verbose = False) \n            acc += history.history['accuracy'][0] \n            loss += history.history['loss'][0]\n            val_acc += history.history['val_accuracy'][0]\n            val_loss += history.history['val_loss'][0]\n            step += 1\n            #if step % 50 == 0:\n            print(\n                    \"Training loss (for one batch) at step %d: %.4f\"\n                    % (step, history.history['loss'][0])\n                )\n            print(\"Seen so far: %s samples\" % ((step) * train_iter.batch_size))\n        acc_epochs.append(acc/training_iters) \n        loss_epochs.append(loss/training_iters) \n        val_acc_epochs.append(val_acc/training_iters)\n        val_loss_epochs.append(val_loss/training_iters)\n    return [acc_epochs,loss_epochs,val_acc_epochs,val_loss_epochs]","execution_count":28,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model1 = Sequential([\n    Embedding(22, 8, input_length=maxlen),  \n    Flatten(),   \n    Dense(num_classes, activation ='softmax') \n]) \n\nmodel1.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"epochs = 12\nbatch_size = 64\nacc_loss = train_val_model(model1,epochs,X_train,y_train,batch_size,X_val,y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = range(1, len(acc_loss) + 1) \nplt.plot(acc_loss[0], 'bo', label='Training acc') \nplt.plot(acc_loss[2], 'r', label='Validation acc') \nplt.legend() \nplt.figure() \nplt.plot(acc_loss[1], 'bo', label='Training loss')  \nplt.plot(acc_loss[3], 'r', label='Validation loss')\nplt.legend() \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model1.evaluate(X_test, y_test)\nprint(\"test loss, test acc:\", results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model2 = Sequential([\n    Embedding(22, 8, input_length=maxlen), \n    Flatten(),\n    Dense(num_classes, activation ='softmax') \n]) \n\nmodel2.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n\nmodel2.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"epochs = 15\nbatch_size = 512\nacc_loss = train_val_model(model2,epochs,X_train,y_train,batch_size,X_val,y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = range(1, len(acc_loss) + 1) \nplt.plot(acc_loss[0], 'bo', label='Training acc') \nplt.plot(acc_loss[2], 'r', label='Validation acc') \nplt.legend() \nplt.figure() \nplt.plot(acc_loss[1], 'bo', label='Training loss')  \nplt.plot(acc_loss[3], 'r', label='Validation loss')\nplt.legend() \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model2.evaluate(X_test, y_test)\nprint(\"test loss, test acc:\", results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model8 = Sequential([\n    Embedding(22, 8, input_length=maxlen),\n    Bidirectional(LSTM(100, return_sequences=True, recurrent_dropout=0.5)),\n    Conv1D(filters=100, kernel_size=5, padding='same', activation='relu') , \n    MaxPooling1D(pool_size=2),\n    \n    Bidirectional(LSTM(100, return_sequences=True, recurrent_dropout=0.5)),\n    Conv1D(filters=100, kernel_size=5, padding='same', activation='relu'),   \n    MaxPooling1D(pool_size=2),\n    \n    Bidirectional(LSTM(100, return_sequences=True, recurrent_dropout=0.5)),\n    Conv1D(filters=100, kernel_size=5, padding='same', activation='relu') ,  \n    MaxPooling1D(pool_size=2),\n    \n    Bidirectional(LSTM(100, return_sequences=True, recurrent_dropout=0.5)),\n    Conv1D(filters=100, kernel_size=5, padding='same', activation='relu') ,  \n    MaxPooling1D(pool_size=2),\n    Bidirectional(LSTM(100, return_sequences=False, recurrent_dropout=0.5)),\n    \n    Dense(num_classes, activation ='softmax') \n]) \n\nmodel8.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n\nmodel8.summary()","execution_count":26,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 200, 8)            176       \n_________________________________________________________________\nbidirectional (Bidirectional (None, 200, 200)          87200     \n_________________________________________________________________\nconv1d (Conv1D)              (None, 200, 100)          100100    \n_________________________________________________________________\nmax_pooling1d (MaxPooling1D) (None, 100, 100)          0         \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 100, 200)          160800    \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 100, 100)          100100    \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 50, 100)           0         \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 50, 200)           160800    \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 50, 100)           100100    \n_________________________________________________________________\nmax_pooling1d_2 (MaxPooling1 (None, 25, 100)           0         \n_________________________________________________________________\nbidirectional_3 (Bidirection (None, 25, 200)           160800    \n_________________________________________________________________\nconv1d_3 (Conv1D)            (None, 25, 100)           100100    \n_________________________________________________________________\nmax_pooling1d_3 (MaxPooling1 (None, 12, 100)           0         \n_________________________________________________________________\nbidirectional_4 (Bidirection (None, 200)               160800    \n_________________________________________________________________\ndense (Dense)                (None, 5770)              1159770   \n=================================================================\nTotal params: 2,290,746\nTrainable params: 2,290,746\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 5\nbatch_size = 64\nacc_loss = train_val_model(model8,epochs,X_train,y_train,batch_size,X_val,y_val)","execution_count":29,"outputs":[{"output_type":"stream","text":"\nEpoch: 0\nTraining loss (for one batch) at step 1: 8.6493\nSeen so far: 64 samples\nTraining loss (for one batch) at step 2: 8.6430\nSeen so far: 128 samples\nTraining loss (for one batch) at step 3: 8.6344\nSeen so far: 192 samples\nTraining loss (for one batch) at step 4: 8.5466\nSeen so far: 256 samples\nTraining loss (for one batch) at step 5: 8.5754\nSeen so far: 320 samples\nTraining loss (for one batch) at step 6: 8.5217\nSeen so far: 384 samples\nTraining loss (for one batch) at step 7: 8.5415\nSeen so far: 448 samples\nTraining loss (for one batch) at step 8: 8.4776\nSeen so far: 512 samples\nTraining loss (for one batch) at step 9: 8.6263\nSeen so far: 576 samples\nTraining loss (for one batch) at step 10: 8.6761\nSeen so far: 640 samples\nTraining loss (for one batch) at step 11: 8.5326\nSeen so far: 704 samples\nTraining loss (for one batch) at step 12: 8.6338\nSeen so far: 768 samples\nTraining loss (for one batch) at step 13: 8.6376\nSeen so far: 832 samples\nTraining loss (for one batch) at step 14: 8.6233\nSeen so far: 896 samples\nTraining loss (for one batch) at step 15: 8.5684\nSeen so far: 960 samples\nTraining loss (for one batch) at step 16: 8.7069\nSeen so far: 1024 samples\nTraining loss (for one batch) at step 17: 8.8229\nSeen so far: 1088 samples\nTraining loss (for one batch) at step 18: 8.6224\nSeen so far: 1152 samples\nTraining loss (for one batch) at step 19: 8.5828\nSeen so far: 1216 samples\nTraining loss (for one batch) at step 20: 8.6213\nSeen so far: 1280 samples\nTraining loss (for one batch) at step 21: 8.6076\nSeen so far: 1344 samples\nTraining loss (for one batch) at step 22: 8.7140\nSeen so far: 1408 samples\nTraining loss (for one batch) at step 23: 8.6753\nSeen so far: 1472 samples\nTraining loss (for one batch) at step 24: 8.5192\nSeen so far: 1536 samples\nTraining loss (for one batch) at step 25: 8.5618\nSeen so far: 1600 samples\nTraining loss (for one batch) at step 26: 8.6182\nSeen so far: 1664 samples\nTraining loss (for one batch) at step 27: 8.6079\nSeen so far: 1728 samples\nTraining loss (for one batch) at step 28: 8.6419\nSeen so far: 1792 samples\nTraining loss (for one batch) at step 29: 8.6348\nSeen so far: 1856 samples\nTraining loss (for one batch) at step 30: 8.6234\nSeen so far: 1920 samples\nTraining loss (for one batch) at step 31: 8.6721\nSeen so far: 1984 samples\nTraining loss (for one batch) at step 32: 8.6298\nSeen so far: 2048 samples\nTraining loss (for one batch) at step 33: 8.6569\nSeen so far: 2112 samples\nTraining loss (for one batch) at step 34: 8.5113\nSeen so far: 2176 samples\nTraining loss (for one batch) at step 35: 8.5247\nSeen so far: 2240 samples\nTraining loss (for one batch) at step 36: 8.6331\nSeen so far: 2304 samples\nTraining loss (for one batch) at step 37: 8.6959\nSeen so far: 2368 samples\nTraining loss (for one batch) at step 38: 8.7080\nSeen so far: 2432 samples\nTraining loss (for one batch) at step 39: 8.6458\nSeen so far: 2496 samples\nTraining loss (for one batch) at step 40: 8.6156\nSeen so far: 2560 samples\nTraining loss (for one batch) at step 41: 8.7206\nSeen so far: 2624 samples\nTraining loss (for one batch) at step 42: 8.6989\nSeen so far: 2688 samples\nTraining loss (for one batch) at step 43: 8.7506\nSeen so far: 2752 samples\nTraining loss (for one batch) at step 44: 8.6360\nSeen so far: 2816 samples\nTraining loss (for one batch) at step 45: 8.6007\nSeen so far: 2880 samples\nTraining loss (for one batch) at step 46: 8.6510\nSeen so far: 2944 samples\nTraining loss (for one batch) at step 47: 8.6322\nSeen so far: 3008 samples\nTraining loss (for one batch) at step 48: 8.6423\nSeen so far: 3072 samples\nTraining loss (for one batch) at step 49: 8.6229\nSeen so far: 3136 samples\nTraining loss (for one batch) at step 50: 8.6702\nSeen so far: 3200 samples\nTraining loss (for one batch) at step 51: 8.6348\nSeen so far: 3264 samples\nTraining loss (for one batch) at step 52: 8.6217\nSeen so far: 3328 samples\nTraining loss (for one batch) at step 53: 8.6370\nSeen so far: 3392 samples\nTraining loss (for one batch) at step 54: 8.6600\nSeen so far: 3456 samples\nTraining loss (for one batch) at step 55: 8.6575\nSeen so far: 3520 samples\nTraining loss (for one batch) at step 56: 8.6401\nSeen so far: 3584 samples\nTraining loss (for one batch) at step 57: 8.6450\nSeen so far: 3648 samples\nTraining loss (for one batch) at step 58: 8.6675\nSeen so far: 3712 samples\nTraining loss (for one batch) at step 59: 8.6230\nSeen so far: 3776 samples\nTraining loss (for one batch) at step 60: 8.6675\nSeen so far: 3840 samples\nTraining loss (for one batch) at step 61: 8.6528\nSeen so far: 3904 samples\nTraining loss (for one batch) at step 62: 8.6778\nSeen so far: 3968 samples\nTraining loss (for one batch) at step 63: 8.6261\nSeen so far: 4032 samples\nTraining loss (for one batch) at step 64: 8.6376\nSeen so far: 4096 samples\nTraining loss (for one batch) at step 65: 8.6662\nSeen so far: 4160 samples\nTraining loss (for one batch) at step 66: 8.6460\nSeen so far: 4224 samples\nTraining loss (for one batch) at step 67: 8.6699\nSeen so far: 4288 samples\nTraining loss (for one batch) at step 68: 8.6140\nSeen so far: 4352 samples\nTraining loss (for one batch) at step 69: 8.6652\nSeen so far: 4416 samples\nTraining loss (for one batch) at step 70: 8.6976\nSeen so far: 4480 samples\nTraining loss (for one batch) at step 71: 8.6128\nSeen so far: 4544 samples\nTraining loss (for one batch) at step 72: 8.6416\nSeen so far: 4608 samples\nTraining loss (for one batch) at step 73: 8.6861\nSeen so far: 4672 samples\nTraining loss (for one batch) at step 74: 8.6480\nSeen so far: 4736 samples\nTraining loss (for one batch) at step 75: 8.6452\nSeen so far: 4800 samples\nTraining loss (for one batch) at step 76: 8.6806\nSeen so far: 4864 samples\nTraining loss (for one batch) at step 77: 8.6025\nSeen so far: 4928 samples\nTraining loss (for one batch) at step 78: 8.6774\nSeen so far: 4992 samples\nTraining loss (for one batch) at step 79: 8.6336\nSeen so far: 5056 samples\nTraining loss (for one batch) at step 80: 8.6665\nSeen so far: 5120 samples\nTraining loss (for one batch) at step 81: 8.6579\nSeen so far: 5184 samples\nTraining loss (for one batch) at step 82: 8.5885\nSeen so far: 5248 samples\nTraining loss (for one batch) at step 83: 8.6074\nSeen so far: 5312 samples\nTraining loss (for one batch) at step 84: 8.6156\nSeen so far: 5376 samples\nTraining loss (for one batch) at step 85: 8.6324\nSeen so far: 5440 samples\nTraining loss (for one batch) at step 86: 8.6659\nSeen so far: 5504 samples\nTraining loss (for one batch) at step 87: 8.6975\nSeen so far: 5568 samples\nTraining loss (for one batch) at step 88: 8.6661\nSeen so far: 5632 samples\nTraining loss (for one batch) at step 89: 8.6568\nSeen so far: 5696 samples\nTraining loss (for one batch) at step 90: 8.6005\nSeen so far: 5760 samples\nTraining loss (for one batch) at step 91: 8.6444\nSeen so far: 5824 samples\nTraining loss (for one batch) at step 92: 8.6311\nSeen so far: 5888 samples\nTraining loss (for one batch) at step 93: 8.6557\nSeen so far: 5952 samples\nTraining loss (for one batch) at step 94: 8.5959\nSeen so far: 6016 samples\nTraining loss (for one batch) at step 95: 8.6988\nSeen so far: 6080 samples\nTraining loss (for one batch) at step 96: 8.6185\nSeen so far: 6144 samples\nTraining loss (for one batch) at step 97: 8.7115\nSeen so far: 6208 samples\nTraining loss (for one batch) at step 98: 8.6075\nSeen so far: 6272 samples\nTraining loss (for one batch) at step 99: 8.6613\nSeen so far: 6336 samples\nTraining loss (for one batch) at step 100: 8.6240\nSeen so far: 6400 samples\nTraining loss (for one batch) at step 101: 8.6176\nSeen so far: 6464 samples\nTraining loss (for one batch) at step 102: 8.5888\nSeen so far: 6528 samples\nTraining loss (for one batch) at step 103: 8.5780\nSeen so far: 6592 samples\nTraining loss (for one batch) at step 104: 8.6028\nSeen so far: 6656 samples\nTraining loss (for one batch) at step 105: 8.7096\nSeen so far: 6720 samples\nTraining loss (for one batch) at step 106: 8.6229\nSeen so far: 6784 samples\nTraining loss (for one batch) at step 107: 8.6230\nSeen so far: 6848 samples\nTraining loss (for one batch) at step 108: 8.7446\nSeen so far: 6912 samples\nTraining loss (for one batch) at step 109: 8.6979\nSeen so far: 6976 samples\nTraining loss (for one batch) at step 110: 8.6747\nSeen so far: 7040 samples\n","name":"stdout"},{"output_type":"stream","text":"Training loss (for one batch) at step 111: 8.6514\nSeen so far: 7104 samples\nTraining loss (for one batch) at step 112: 8.7290\nSeen so far: 7168 samples\nTraining loss (for one batch) at step 113: 8.6727\nSeen so far: 7232 samples\nTraining loss (for one batch) at step 114: 8.5822\nSeen so far: 7296 samples\nTraining loss (for one batch) at step 115: 8.6393\nSeen so far: 7360 samples\nTraining loss (for one batch) at step 116: 8.6088\nSeen so far: 7424 samples\nTraining loss (for one batch) at step 117: 8.6448\nSeen so far: 7488 samples\nTraining loss (for one batch) at step 118: 8.6608\nSeen so far: 7552 samples\nTraining loss (for one batch) at step 119: 8.6230\nSeen so far: 7616 samples\nTraining loss (for one batch) at step 120: 8.6913\nSeen so far: 7680 samples\nTraining loss (for one batch) at step 121: 8.6251\nSeen so far: 7744 samples\nTraining loss (for one batch) at step 122: 8.6104\nSeen so far: 7808 samples\nTraining loss (for one batch) at step 123: 8.6508\nSeen so far: 7872 samples\nTraining loss (for one batch) at step 124: 8.6029\nSeen so far: 7936 samples\nTraining loss (for one batch) at step 125: 8.6647\nSeen so far: 8000 samples\nTraining loss (for one batch) at step 126: 8.6367\nSeen so far: 8064 samples\nTraining loss (for one batch) at step 127: 8.5854\nSeen so far: 8128 samples\nTraining loss (for one batch) at step 128: 8.6377\nSeen so far: 8192 samples\nTraining loss (for one batch) at step 129: 8.6180\nSeen so far: 8256 samples\nTraining loss (for one batch) at step 130: 8.6038\nSeen so far: 8320 samples\nTraining loss (for one batch) at step 131: 8.6737\nSeen so far: 8384 samples\nTraining loss (for one batch) at step 132: 8.6656\nSeen so far: 8448 samples\nTraining loss (for one batch) at step 133: 8.6312\nSeen so far: 8512 samples\nTraining loss (for one batch) at step 134: 8.6038\nSeen so far: 8576 samples\nTraining loss (for one batch) at step 135: 8.6354\nSeen so far: 8640 samples\nTraining loss (for one batch) at step 136: 8.6499\nSeen so far: 8704 samples\nTraining loss (for one batch) at step 137: 8.6524\nSeen so far: 8768 samples\nTraining loss (for one batch) at step 138: 8.6410\nSeen so far: 8832 samples\nTraining loss (for one batch) at step 139: 8.6133\nSeen so far: 8896 samples\nTraining loss (for one batch) at step 140: 8.6906\nSeen so far: 8960 samples\nTraining loss (for one batch) at step 141: 8.5792\nSeen so far: 9024 samples\nTraining loss (for one batch) at step 142: 8.6609\nSeen so far: 9088 samples\nTraining loss (for one batch) at step 143: 8.6453\nSeen so far: 9152 samples\nTraining loss (for one batch) at step 144: 8.6588\nSeen so far: 9216 samples\nTraining loss (for one batch) at step 145: 8.6851\nSeen so far: 9280 samples\nTraining loss (for one batch) at step 146: 8.6252\nSeen so far: 9344 samples\nTraining loss (for one batch) at step 147: 8.6483\nSeen so far: 9408 samples\nTraining loss (for one batch) at step 148: 8.6222\nSeen so far: 9472 samples\nTraining loss (for one batch) at step 149: 8.6295\nSeen so far: 9536 samples\nTraining loss (for one batch) at step 150: 8.6344\nSeen so far: 9600 samples\nTraining loss (for one batch) at step 151: 8.6368\nSeen so far: 9664 samples\nTraining loss (for one batch) at step 152: 8.5197\nSeen so far: 9728 samples\nTraining loss (for one batch) at step 153: 8.5487\nSeen so far: 9792 samples\n\nEpoch: 1\nTraining loss (for one batch) at step 1: 8.4464\nSeen so far: 64 samples\nTraining loss (for one batch) at step 2: 8.4957\nSeen so far: 128 samples\nTraining loss (for one batch) at step 3: 8.5036\nSeen so far: 192 samples\nTraining loss (for one batch) at step 4: 8.5575\nSeen so far: 256 samples\nTraining loss (for one batch) at step 5: 8.4762\nSeen so far: 320 samples\nTraining loss (for one batch) at step 6: 8.4105\nSeen so far: 384 samples\nTraining loss (for one batch) at step 7: 8.4763\nSeen so far: 448 samples\nTraining loss (for one batch) at step 8: 8.4351\nSeen so far: 512 samples\nTraining loss (for one batch) at step 9: 8.3372\nSeen so far: 576 samples\nTraining loss (for one batch) at step 10: 8.4241\nSeen so far: 640 samples\nTraining loss (for one batch) at step 11: 8.4279\nSeen so far: 704 samples\nTraining loss (for one batch) at step 12: 8.5110\nSeen so far: 768 samples\nTraining loss (for one batch) at step 13: 8.4991\nSeen so far: 832 samples\nTraining loss (for one batch) at step 14: 8.3086\nSeen so far: 896 samples\nTraining loss (for one batch) at step 15: 8.4355\nSeen so far: 960 samples\nTraining loss (for one batch) at step 16: 8.5546\nSeen so far: 1024 samples\nTraining loss (for one batch) at step 17: 8.5290\nSeen so far: 1088 samples\nTraining loss (for one batch) at step 18: 8.3609\nSeen so far: 1152 samples\nTraining loss (for one batch) at step 19: 8.3128\nSeen so far: 1216 samples\nTraining loss (for one batch) at step 20: 8.2955\nSeen so far: 1280 samples\nTraining loss (for one batch) at step 21: 8.5402\nSeen so far: 1344 samples\nTraining loss (for one batch) at step 22: 8.4228\nSeen so far: 1408 samples\nTraining loss (for one batch) at step 23: 8.4786\nSeen so far: 1472 samples\nTraining loss (for one batch) at step 24: 8.4127\nSeen so far: 1536 samples\nTraining loss (for one batch) at step 25: 8.4484\nSeen so far: 1600 samples\nTraining loss (for one batch) at step 26: 8.3881\nSeen so far: 1664 samples\nTraining loss (for one batch) at step 27: 8.4258\nSeen so far: 1728 samples\nTraining loss (for one batch) at step 28: 8.5756\nSeen so far: 1792 samples\nTraining loss (for one batch) at step 29: 8.3636\nSeen so far: 1856 samples\nTraining loss (for one batch) at step 30: 8.3787\nSeen so far: 1920 samples\nTraining loss (for one batch) at step 31: 8.4061\nSeen so far: 1984 samples\nTraining loss (for one batch) at step 32: 8.5564\nSeen so far: 2048 samples\nTraining loss (for one batch) at step 33: 8.4026\nSeen so far: 2112 samples\nTraining loss (for one batch) at step 34: 8.4858\nSeen so far: 2176 samples\nTraining loss (for one batch) at step 35: 8.3634\nSeen so far: 2240 samples\nTraining loss (for one batch) at step 36: 8.5054\nSeen so far: 2304 samples\nTraining loss (for one batch) at step 37: 8.4493\nSeen so far: 2368 samples\nTraining loss (for one batch) at step 38: 8.5062\nSeen so far: 2432 samples\nTraining loss (for one batch) at step 39: 8.4310\nSeen so far: 2496 samples\nTraining loss (for one batch) at step 40: 8.4248\nSeen so far: 2560 samples\nTraining loss (for one batch) at step 41: 8.4254\nSeen so far: 2624 samples\nTraining loss (for one batch) at step 42: 8.5651\nSeen so far: 2688 samples\nTraining loss (for one batch) at step 43: 8.4284\nSeen so far: 2752 samples\nTraining loss (for one batch) at step 44: 8.4047\nSeen so far: 2816 samples\nTraining loss (for one batch) at step 45: 8.3585\nSeen so far: 2880 samples\nTraining loss (for one batch) at step 46: 8.3519\nSeen so far: 2944 samples\nTraining loss (for one batch) at step 47: 8.3174\nSeen so far: 3008 samples\nTraining loss (for one batch) at step 48: 8.5135\nSeen so far: 3072 samples\nTraining loss (for one batch) at step 49: 8.4704\nSeen so far: 3136 samples\nTraining loss (for one batch) at step 50: 8.3147\nSeen so far: 3200 samples\nTraining loss (for one batch) at step 51: 8.4899\nSeen so far: 3264 samples\nTraining loss (for one batch) at step 52: 8.3646\nSeen so far: 3328 samples\nTraining loss (for one batch) at step 53: 8.5495\nSeen so far: 3392 samples\nTraining loss (for one batch) at step 54: 8.3952\nSeen so far: 3456 samples\nTraining loss (for one batch) at step 55: 8.3204\nSeen so far: 3520 samples\nTraining loss (for one batch) at step 56: 8.5280\nSeen so far: 3584 samples\nTraining loss (for one batch) at step 57: 8.4000\nSeen so far: 3648 samples\nTraining loss (for one batch) at step 58: 8.3304\nSeen so far: 3712 samples\nTraining loss (for one batch) at step 59: 8.4583\nSeen so far: 3776 samples\nTraining loss (for one batch) at step 60: 8.3640\nSeen so far: 3840 samples\nTraining loss (for one batch) at step 61: 8.6323\nSeen so far: 3904 samples\nTraining loss (for one batch) at step 62: 8.4419\nSeen so far: 3968 samples\nTraining loss (for one batch) at step 63: 8.4064\nSeen so far: 4032 samples\nTraining loss (for one batch) at step 64: 8.4856\nSeen so far: 4096 samples\nTraining loss (for one batch) at step 65: 8.3553\nSeen so far: 4160 samples\nTraining loss (for one batch) at step 66: 8.5320\nSeen so far: 4224 samples\n","name":"stdout"},{"output_type":"stream","text":"Training loss (for one batch) at step 67: 8.4465\nSeen so far: 4288 samples\nTraining loss (for one batch) at step 68: 8.5698\nSeen so far: 4352 samples\nTraining loss (for one batch) at step 69: 8.2513\nSeen so far: 4416 samples\nTraining loss (for one batch) at step 70: 8.4537\nSeen so far: 4480 samples\nTraining loss (for one batch) at step 71: 8.4345\nSeen so far: 4544 samples\nTraining loss (for one batch) at step 72: 8.5276\nSeen so far: 4608 samples\nTraining loss (for one batch) at step 73: 8.6159\nSeen so far: 4672 samples\nTraining loss (for one batch) at step 74: 8.4738\nSeen so far: 4736 samples\nTraining loss (for one batch) at step 75: 8.5570\nSeen so far: 4800 samples\nTraining loss (for one batch) at step 76: 8.4660\nSeen so far: 4864 samples\nTraining loss (for one batch) at step 77: 8.5815\nSeen so far: 4928 samples\nTraining loss (for one batch) at step 78: 8.4032\nSeen so far: 4992 samples\nTraining loss (for one batch) at step 79: 8.4266\nSeen so far: 5056 samples\nTraining loss (for one batch) at step 80: 8.5317\nSeen so far: 5120 samples\nTraining loss (for one batch) at step 81: 8.4453\nSeen so far: 5184 samples\nTraining loss (for one batch) at step 82: 8.4341\nSeen so far: 5248 samples\nTraining loss (for one batch) at step 83: 8.5190\nSeen so far: 5312 samples\nTraining loss (for one batch) at step 84: 8.5476\nSeen so far: 5376 samples\nTraining loss (for one batch) at step 85: 8.4866\nSeen so far: 5440 samples\nTraining loss (for one batch) at step 86: 8.5267\nSeen so far: 5504 samples\nTraining loss (for one batch) at step 87: 8.4016\nSeen so far: 5568 samples\nTraining loss (for one batch) at step 88: 8.4791\nSeen so far: 5632 samples\nTraining loss (for one batch) at step 89: 8.3828\nSeen so far: 5696 samples\nTraining loss (for one batch) at step 90: 8.6689\nSeen so far: 5760 samples\nTraining loss (for one batch) at step 91: 8.6034\nSeen so far: 5824 samples\nTraining loss (for one batch) at step 92: 8.3609\nSeen so far: 5888 samples\nTraining loss (for one batch) at step 93: 8.5760\nSeen so far: 5952 samples\nTraining loss (for one batch) at step 94: 8.4635\nSeen so far: 6016 samples\nTraining loss (for one batch) at step 95: 8.4291\nSeen so far: 6080 samples\nTraining loss (for one batch) at step 96: 8.5070\nSeen so far: 6144 samples\nTraining loss (for one batch) at step 97: 8.6380\nSeen so far: 6208 samples\nTraining loss (for one batch) at step 98: 8.3912\nSeen so far: 6272 samples\nTraining loss (for one batch) at step 99: 8.3245\nSeen so far: 6336 samples\nTraining loss (for one batch) at step 100: 8.5234\nSeen so far: 6400 samples\nTraining loss (for one batch) at step 101: 8.3761\nSeen so far: 6464 samples\nTraining loss (for one batch) at step 102: 8.5435\nSeen so far: 6528 samples\nTraining loss (for one batch) at step 103: 8.3626\nSeen so far: 6592 samples\nTraining loss (for one batch) at step 104: 8.5421\nSeen so far: 6656 samples\nTraining loss (for one batch) at step 105: 8.5645\nSeen so far: 6720 samples\nTraining loss (for one batch) at step 106: 8.5692\nSeen so far: 6784 samples\nTraining loss (for one batch) at step 107: 8.4610\nSeen so far: 6848 samples\nTraining loss (for one batch) at step 108: 8.6012\nSeen so far: 6912 samples\nTraining loss (for one batch) at step 109: 8.6413\nSeen so far: 6976 samples\nTraining loss (for one batch) at step 110: 8.5640\nSeen so far: 7040 samples\nTraining loss (for one batch) at step 111: 8.5023\nSeen so far: 7104 samples\nTraining loss (for one batch) at step 112: 8.4034\nSeen so far: 7168 samples\nTraining loss (for one batch) at step 113: 8.6191\nSeen so far: 7232 samples\nTraining loss (for one batch) at step 114: 8.5820\nSeen so far: 7296 samples\nTraining loss (for one batch) at step 115: 8.5486\nSeen so far: 7360 samples\nTraining loss (for one batch) at step 116: 8.4618\nSeen so far: 7424 samples\nTraining loss (for one batch) at step 117: 8.6420\nSeen so far: 7488 samples\nTraining loss (for one batch) at step 118: 8.5790\nSeen so far: 7552 samples\nTraining loss (for one batch) at step 119: 8.4868\nSeen so far: 7616 samples\nTraining loss (for one batch) at step 120: 8.4737\nSeen so far: 7680 samples\nTraining loss (for one batch) at step 121: 8.4867\nSeen so far: 7744 samples\nTraining loss (for one batch) at step 122: 8.4583\nSeen so far: 7808 samples\nTraining loss (for one batch) at step 123: 8.4502\nSeen so far: 7872 samples\nTraining loss (for one batch) at step 124: 8.4223\nSeen so far: 7936 samples\nTraining loss (for one batch) at step 125: 8.5071\nSeen so far: 8000 samples\nTraining loss (for one batch) at step 126: 8.3995\nSeen so far: 8064 samples\nTraining loss (for one batch) at step 127: 8.4256\nSeen so far: 8128 samples\nTraining loss (for one batch) at step 128: 8.6068\nSeen so far: 8192 samples\nTraining loss (for one batch) at step 129: 8.4935\nSeen so far: 8256 samples\nTraining loss (for one batch) at step 130: 8.5323\nSeen so far: 8320 samples\nTraining loss (for one batch) at step 131: 8.4594\nSeen so far: 8384 samples\nTraining loss (for one batch) at step 132: 8.4951\nSeen so far: 8448 samples\nTraining loss (for one batch) at step 133: 8.4637\nSeen so far: 8512 samples\nTraining loss (for one batch) at step 134: 8.4521\nSeen so far: 8576 samples\nTraining loss (for one batch) at step 135: 8.4280\nSeen so far: 8640 samples\nTraining loss (for one batch) at step 136: 8.4217\nSeen so far: 8704 samples\nTraining loss (for one batch) at step 137: 8.6350\nSeen so far: 8768 samples\nTraining loss (for one batch) at step 138: 8.3836\nSeen so far: 8832 samples\nTraining loss (for one batch) at step 139: 8.4059\nSeen so far: 8896 samples\nTraining loss (for one batch) at step 140: 8.5377\nSeen so far: 8960 samples\nTraining loss (for one batch) at step 141: 8.5351\nSeen so far: 9024 samples\nTraining loss (for one batch) at step 142: 8.5699\nSeen so far: 9088 samples\nTraining loss (for one batch) at step 143: 8.5350\nSeen so far: 9152 samples\nTraining loss (for one batch) at step 144: 8.4659\nSeen so far: 9216 samples\nTraining loss (for one batch) at step 145: 8.4357\nSeen so far: 9280 samples\nTraining loss (for one batch) at step 146: 8.4546\nSeen so far: 9344 samples\nTraining loss (for one batch) at step 147: 8.2826\nSeen so far: 9408 samples\nTraining loss (for one batch) at step 148: 8.5454\nSeen so far: 9472 samples\nTraining loss (for one batch) at step 149: 8.5006\nSeen so far: 9536 samples\nTraining loss (for one batch) at step 150: 8.5546\nSeen so far: 9600 samples\nTraining loss (for one batch) at step 151: 8.3400\nSeen so far: 9664 samples\nTraining loss (for one batch) at step 152: 8.3256\nSeen so far: 9728 samples\nTraining loss (for one batch) at step 153: 8.3076\nSeen so far: 9792 samples\n\nEpoch: 2\nTraining loss (for one batch) at step 1: 8.2813\nSeen so far: 64 samples\nTraining loss (for one batch) at step 2: 8.4236\nSeen so far: 128 samples\nTraining loss (for one batch) at step 3: 8.4597\nSeen so far: 192 samples\nTraining loss (for one batch) at step 4: 8.3125\nSeen so far: 256 samples\nTraining loss (for one batch) at step 5: 8.3943\nSeen so far: 320 samples\nTraining loss (for one batch) at step 6: 8.3150\nSeen so far: 384 samples\nTraining loss (for one batch) at step 7: 8.1479\nSeen so far: 448 samples\nTraining loss (for one batch) at step 8: 8.2833\nSeen so far: 512 samples\nTraining loss (for one batch) at step 9: 8.3279\nSeen so far: 576 samples\nTraining loss (for one batch) at step 10: 8.2252\nSeen so far: 640 samples\nTraining loss (for one batch) at step 11: 8.4916\nSeen so far: 704 samples\nTraining loss (for one batch) at step 12: 8.4212\nSeen so far: 768 samples\nTraining loss (for one batch) at step 13: 8.2295\nSeen so far: 832 samples\nTraining loss (for one batch) at step 14: 8.2710\nSeen so far: 896 samples\nTraining loss (for one batch) at step 15: 8.3654\nSeen so far: 960 samples\nTraining loss (for one batch) at step 16: 8.3810\nSeen so far: 1024 samples\nTraining loss (for one batch) at step 17: 8.4223\nSeen so far: 1088 samples\nTraining loss (for one batch) at step 18: 8.3189\nSeen so far: 1152 samples\nTraining loss (for one batch) at step 19: 8.2392\nSeen so far: 1216 samples\nTraining loss (for one batch) at step 20: 8.3384\nSeen so far: 1280 samples\nTraining loss (for one batch) at step 21: 8.4422\nSeen so far: 1344 samples\nTraining loss (for one batch) at step 22: 8.3625\nSeen so far: 1408 samples\n","name":"stdout"},{"output_type":"stream","text":"Training loss (for one batch) at step 23: 8.4611\nSeen so far: 1472 samples\nTraining loss (for one batch) at step 24: 8.2962\nSeen so far: 1536 samples\nTraining loss (for one batch) at step 25: 8.3937\nSeen so far: 1600 samples\nTraining loss (for one batch) at step 26: 8.3171\nSeen so far: 1664 samples\nTraining loss (for one batch) at step 27: 8.2541\nSeen so far: 1728 samples\nTraining loss (for one batch) at step 28: 8.3566\nSeen so far: 1792 samples\nTraining loss (for one batch) at step 29: 8.2630\nSeen so far: 1856 samples\nTraining loss (for one batch) at step 30: 8.3742\nSeen so far: 1920 samples\nTraining loss (for one batch) at step 31: 8.2085\nSeen so far: 1984 samples\nTraining loss (for one batch) at step 32: 8.4057\nSeen so far: 2048 samples\nTraining loss (for one batch) at step 33: 8.2180\nSeen so far: 2112 samples\nTraining loss (for one batch) at step 34: 8.2622\nSeen so far: 2176 samples\nTraining loss (for one batch) at step 35: 8.5196\nSeen so far: 2240 samples\nTraining loss (for one batch) at step 36: 8.2475\nSeen so far: 2304 samples\nTraining loss (for one batch) at step 37: 8.3008\nSeen so far: 2368 samples\nTraining loss (for one batch) at step 38: 8.2100\nSeen so far: 2432 samples\nTraining loss (for one batch) at step 39: 8.5262\nSeen so far: 2496 samples\nTraining loss (for one batch) at step 40: 8.3055\nSeen so far: 2560 samples\nTraining loss (for one batch) at step 41: 8.2331\nSeen so far: 2624 samples\nTraining loss (for one batch) at step 42: 8.5219\nSeen so far: 2688 samples\nTraining loss (for one batch) at step 43: 8.4441\nSeen so far: 2752 samples\nTraining loss (for one batch) at step 44: 8.3490\nSeen so far: 2816 samples\nTraining loss (for one batch) at step 45: 8.3818\nSeen so far: 2880 samples\nTraining loss (for one batch) at step 46: 8.4343\nSeen so far: 2944 samples\nTraining loss (for one batch) at step 47: 8.2028\nSeen so far: 3008 samples\nTraining loss (for one batch) at step 48: 8.4357\nSeen so far: 3072 samples\nTraining loss (for one batch) at step 49: 8.3692\nSeen so far: 3136 samples\nTraining loss (for one batch) at step 50: 8.3041\nSeen so far: 3200 samples\nTraining loss (for one batch) at step 51: 8.3797\nSeen so far: 3264 samples\nTraining loss (for one batch) at step 52: 8.3547\nSeen so far: 3328 samples\nTraining loss (for one batch) at step 53: 8.2362\nSeen so far: 3392 samples\nTraining loss (for one batch) at step 54: 8.4089\nSeen so far: 3456 samples\nTraining loss (for one batch) at step 55: 8.4680\nSeen so far: 3520 samples\nTraining loss (for one batch) at step 56: 8.3116\nSeen so far: 3584 samples\nTraining loss (for one batch) at step 57: 8.3170\nSeen so far: 3648 samples\nTraining loss (for one batch) at step 58: 8.3791\nSeen so far: 3712 samples\nTraining loss (for one batch) at step 59: 8.3326\nSeen so far: 3776 samples\nTraining loss (for one batch) at step 60: 8.4310\nSeen so far: 3840 samples\nTraining loss (for one batch) at step 61: 8.2891\nSeen so far: 3904 samples\nTraining loss (for one batch) at step 62: 8.3652\nSeen so far: 3968 samples\nTraining loss (for one batch) at step 63: 8.4512\nSeen so far: 4032 samples\nTraining loss (for one batch) at step 64: 8.5076\nSeen so far: 4096 samples\nTraining loss (for one batch) at step 65: 8.2875\nSeen so far: 4160 samples\nTraining loss (for one batch) at step 66: 8.5578\nSeen so far: 4224 samples\nTraining loss (for one batch) at step 67: 8.3037\nSeen so far: 4288 samples\nTraining loss (for one batch) at step 68: 8.2248\nSeen so far: 4352 samples\nTraining loss (for one batch) at step 69: 8.3830\nSeen so far: 4416 samples\nTraining loss (for one batch) at step 70: 8.4116\nSeen so far: 4480 samples\nTraining loss (for one batch) at step 71: 8.3032\nSeen so far: 4544 samples\nTraining loss (for one batch) at step 72: 8.5561\nSeen so far: 4608 samples\nTraining loss (for one batch) at step 73: 8.4612\nSeen so far: 4672 samples\nTraining loss (for one batch) at step 74: 8.4936\nSeen so far: 4736 samples\nTraining loss (for one batch) at step 75: 8.5084\nSeen so far: 4800 samples\nTraining loss (for one batch) at step 76: 8.3225\nSeen so far: 4864 samples\nTraining loss (for one batch) at step 77: 8.3429\nSeen so far: 4928 samples\nTraining loss (for one batch) at step 78: 8.3251\nSeen so far: 4992 samples\nTraining loss (for one batch) at step 79: 8.4560\nSeen so far: 5056 samples\nTraining loss (for one batch) at step 80: 8.4148\nSeen so far: 5120 samples\nTraining loss (for one batch) at step 81: 8.2782\nSeen so far: 5184 samples\nTraining loss (for one batch) at step 82: 8.3298\nSeen so far: 5248 samples\nTraining loss (for one batch) at step 83: 8.3403\nSeen so far: 5312 samples\nTraining loss (for one batch) at step 84: 8.3264\nSeen so far: 5376 samples\nTraining loss (for one batch) at step 85: 8.4882\nSeen so far: 5440 samples\nTraining loss (for one batch) at step 86: 8.3690\nSeen so far: 5504 samples\nTraining loss (for one batch) at step 87: 8.4086\nSeen so far: 5568 samples\nTraining loss (for one batch) at step 88: 8.3850\nSeen so far: 5632 samples\nTraining loss (for one batch) at step 89: 8.4014\nSeen so far: 5696 samples\nTraining loss (for one batch) at step 90: 8.4495\nSeen so far: 5760 samples\nTraining loss (for one batch) at step 91: 8.4215\nSeen so far: 5824 samples\nTraining loss (for one batch) at step 92: 8.2850\nSeen so far: 5888 samples\nTraining loss (for one batch) at step 93: 8.3574\nSeen so far: 5952 samples\nTraining loss (for one batch) at step 94: 8.5333\nSeen so far: 6016 samples\nTraining loss (for one batch) at step 95: 8.5137\nSeen so far: 6080 samples\nTraining loss (for one batch) at step 96: 8.4040\nSeen so far: 6144 samples\nTraining loss (for one batch) at step 97: 8.4895\nSeen so far: 6208 samples\nTraining loss (for one batch) at step 98: 8.4875\nSeen so far: 6272 samples\nTraining loss (for one batch) at step 99: 8.4936\nSeen so far: 6336 samples\nTraining loss (for one batch) at step 100: 8.4750\nSeen so far: 6400 samples\nTraining loss (for one batch) at step 101: 8.3995\nSeen so far: 6464 samples\nTraining loss (for one batch) at step 102: 8.4032\nSeen so far: 6528 samples\nTraining loss (for one batch) at step 103: 8.2468\nSeen so far: 6592 samples\nTraining loss (for one batch) at step 104: 8.4644\nSeen so far: 6656 samples\nTraining loss (for one batch) at step 105: 8.3639\nSeen so far: 6720 samples\nTraining loss (for one batch) at step 106: 8.2910\nSeen so far: 6784 samples\nTraining loss (for one batch) at step 107: 8.5554\nSeen so far: 6848 samples\nTraining loss (for one batch) at step 108: 8.5014\nSeen so far: 6912 samples\nTraining loss (for one batch) at step 109: 8.4320\nSeen so far: 6976 samples\nTraining loss (for one batch) at step 110: 8.4581\nSeen so far: 7040 samples\nTraining loss (for one batch) at step 111: 8.4101\nSeen so far: 7104 samples\nTraining loss (for one batch) at step 112: 8.2616\nSeen so far: 7168 samples\nTraining loss (for one batch) at step 113: 8.3766\nSeen so far: 7232 samples\nTraining loss (for one batch) at step 114: 8.4096\nSeen so far: 7296 samples\nTraining loss (for one batch) at step 115: 8.2969\nSeen so far: 7360 samples\nTraining loss (for one batch) at step 116: 8.3745\nSeen so far: 7424 samples\nTraining loss (for one batch) at step 117: 8.4368\nSeen so far: 7488 samples\nTraining loss (for one batch) at step 118: 8.4738\nSeen so far: 7552 samples\nTraining loss (for one batch) at step 119: 8.5285\nSeen so far: 7616 samples\nTraining loss (for one batch) at step 120: 8.4062\nSeen so far: 7680 samples\nTraining loss (for one batch) at step 121: 8.2991\nSeen so far: 7744 samples\nTraining loss (for one batch) at step 122: 8.3164\nSeen so far: 7808 samples\nTraining loss (for one batch) at step 123: 8.4131\nSeen so far: 7872 samples\nTraining loss (for one batch) at step 124: 8.4215\nSeen so far: 7936 samples\nTraining loss (for one batch) at step 125: 8.2970\nSeen so far: 8000 samples\nTraining loss (for one batch) at step 126: 8.4179\nSeen so far: 8064 samples\nTraining loss (for one batch) at step 127: 8.5101\nSeen so far: 8128 samples\nTraining loss (for one batch) at step 128: 8.4372\nSeen so far: 8192 samples\nTraining loss (for one batch) at step 129: 8.4463\nSeen so far: 8256 samples\nTraining loss (for one batch) at step 130: 8.3238\nSeen so far: 8320 samples\nTraining loss (for one batch) at step 131: 8.3049\nSeen so far: 8384 samples\n","name":"stdout"},{"output_type":"stream","text":"Training loss (for one batch) at step 132: 8.4452\nSeen so far: 8448 samples\nTraining loss (for one batch) at step 133: 8.3691\nSeen so far: 8512 samples\nTraining loss (for one batch) at step 134: 8.4781\nSeen so far: 8576 samples\nTraining loss (for one batch) at step 135: 8.3620\nSeen so far: 8640 samples\nTraining loss (for one batch) at step 136: 8.3457\nSeen so far: 8704 samples\nTraining loss (for one batch) at step 137: 8.4772\nSeen so far: 8768 samples\nTraining loss (for one batch) at step 138: 8.3663\nSeen so far: 8832 samples\nTraining loss (for one batch) at step 139: 8.2538\nSeen so far: 8896 samples\nTraining loss (for one batch) at step 140: 8.3502\nSeen so far: 8960 samples\nTraining loss (for one batch) at step 141: 8.3484\nSeen so far: 9024 samples\nTraining loss (for one batch) at step 142: 8.4578\nSeen so far: 9088 samples\nTraining loss (for one batch) at step 143: 8.2850\nSeen so far: 9152 samples\nTraining loss (for one batch) at step 144: 8.1651\nSeen so far: 9216 samples\nTraining loss (for one batch) at step 145: 8.3989\nSeen so far: 9280 samples\nTraining loss (for one batch) at step 146: 8.2343\nSeen so far: 9344 samples\nTraining loss (for one batch) at step 147: 8.5043\nSeen so far: 9408 samples\nTraining loss (for one batch) at step 148: 8.2866\nSeen so far: 9472 samples\nTraining loss (for one batch) at step 149: 8.2965\nSeen so far: 9536 samples\nTraining loss (for one batch) at step 150: 8.3175\nSeen so far: 9600 samples\nTraining loss (for one batch) at step 151: 8.3864\nSeen so far: 9664 samples\nTraining loss (for one batch) at step 152: 8.3261\nSeen so far: 9728 samples\nTraining loss (for one batch) at step 153: 8.1710\nSeen so far: 9792 samples\n\nEpoch: 3\nTraining loss (for one batch) at step 1: 8.1330\nSeen so far: 64 samples\nTraining loss (for one batch) at step 2: 8.5235\nSeen so far: 128 samples\nTraining loss (for one batch) at step 3: 8.3262\nSeen so far: 192 samples\nTraining loss (for one batch) at step 4: 8.3032\nSeen so far: 256 samples\nTraining loss (for one batch) at step 5: 8.3856\nSeen so far: 320 samples\nTraining loss (for one batch) at step 6: 8.1473\nSeen so far: 384 samples\nTraining loss (for one batch) at step 7: 8.1665\nSeen so far: 448 samples\nTraining loss (for one batch) at step 8: 8.2925\nSeen so far: 512 samples\nTraining loss (for one batch) at step 9: 8.2946\nSeen so far: 576 samples\nTraining loss (for one batch) at step 10: 8.2224\nSeen so far: 640 samples\nTraining loss (for one batch) at step 11: 8.2742\nSeen so far: 704 samples\nTraining loss (for one batch) at step 12: 8.2935\nSeen so far: 768 samples\nTraining loss (for one batch) at step 13: 8.3973\nSeen so far: 832 samples\nTraining loss (for one batch) at step 14: 8.2388\nSeen so far: 896 samples\nTraining loss (for one batch) at step 15: 8.3799\nSeen so far: 960 samples\nTraining loss (for one batch) at step 16: 8.0613\nSeen so far: 1024 samples\nTraining loss (for one batch) at step 17: 8.3878\nSeen so far: 1088 samples\nTraining loss (for one batch) at step 18: 8.1672\nSeen so far: 1152 samples\nTraining loss (for one batch) at step 19: 8.2593\nSeen so far: 1216 samples\nTraining loss (for one batch) at step 20: 8.4010\nSeen so far: 1280 samples\nTraining loss (for one batch) at step 21: 8.4248\nSeen so far: 1344 samples\nTraining loss (for one batch) at step 22: 8.4120\nSeen so far: 1408 samples\nTraining loss (for one batch) at step 23: 8.3381\nSeen so far: 1472 samples\nTraining loss (for one batch) at step 24: 8.3417\nSeen so far: 1536 samples\nTraining loss (for one batch) at step 25: 8.3773\nSeen so far: 1600 samples\nTraining loss (for one batch) at step 26: 8.3402\nSeen so far: 1664 samples\nTraining loss (for one batch) at step 27: 8.2674\nSeen so far: 1728 samples\nTraining loss (for one batch) at step 28: 8.2568\nSeen so far: 1792 samples\nTraining loss (for one batch) at step 29: 8.3901\nSeen so far: 1856 samples\nTraining loss (for one batch) at step 30: 8.2864\nSeen so far: 1920 samples\nTraining loss (for one batch) at step 31: 8.3064\nSeen so far: 1984 samples\nTraining loss (for one batch) at step 32: 8.2507\nSeen so far: 2048 samples\nTraining loss (for one batch) at step 33: 8.2302\nSeen so far: 2112 samples\nTraining loss (for one batch) at step 34: 8.4068\nSeen so far: 2176 samples\nTraining loss (for one batch) at step 35: 8.3185\nSeen so far: 2240 samples\nTraining loss (for one batch) at step 36: 8.3068\nSeen so far: 2304 samples\nTraining loss (for one batch) at step 37: 8.4252\nSeen so far: 2368 samples\nTraining loss (for one batch) at step 38: 8.2917\nSeen so far: 2432 samples\nTraining loss (for one batch) at step 39: 8.3371\nSeen so far: 2496 samples\nTraining loss (for one batch) at step 40: 8.4019\nSeen so far: 2560 samples\nTraining loss (for one batch) at step 41: 8.2984\nSeen so far: 2624 samples\nTraining loss (for one batch) at step 42: 8.3019\nSeen so far: 2688 samples\nTraining loss (for one batch) at step 43: 8.2750\nSeen so far: 2752 samples\nTraining loss (for one batch) at step 44: 8.3787\nSeen so far: 2816 samples\nTraining loss (for one batch) at step 45: 8.1564\nSeen so far: 2880 samples\nTraining loss (for one batch) at step 46: 8.3115\nSeen so far: 2944 samples\nTraining loss (for one batch) at step 47: 8.2960\nSeen so far: 3008 samples\nTraining loss (for one batch) at step 48: 8.2207\nSeen so far: 3072 samples\nTraining loss (for one batch) at step 49: 8.3782\nSeen so far: 3136 samples\nTraining loss (for one batch) at step 50: 8.4833\nSeen so far: 3200 samples\nTraining loss (for one batch) at step 51: 8.3911\nSeen so far: 3264 samples\nTraining loss (for one batch) at step 52: 8.2925\nSeen so far: 3328 samples\nTraining loss (for one batch) at step 53: 8.4153\nSeen so far: 3392 samples\nTraining loss (for one batch) at step 54: 8.1803\nSeen so far: 3456 samples\nTraining loss (for one batch) at step 55: 8.3640\nSeen so far: 3520 samples\nTraining loss (for one batch) at step 56: 8.4515\nSeen so far: 3584 samples\nTraining loss (for one batch) at step 57: 8.4067\nSeen so far: 3648 samples\nTraining loss (for one batch) at step 58: 8.3601\nSeen so far: 3712 samples\nTraining loss (for one batch) at step 59: 8.4520\nSeen so far: 3776 samples\nTraining loss (for one batch) at step 60: 8.3791\nSeen so far: 3840 samples\nTraining loss (for one batch) at step 61: 8.4706\nSeen so far: 3904 samples\nTraining loss (for one batch) at step 62: 8.4234\nSeen so far: 3968 samples\nTraining loss (for one batch) at step 63: 8.3600\nSeen so far: 4032 samples\nTraining loss (for one batch) at step 64: 8.4561\nSeen so far: 4096 samples\nTraining loss (for one batch) at step 65: 8.4653\nSeen so far: 4160 samples\nTraining loss (for one batch) at step 66: 8.3212\nSeen so far: 4224 samples\nTraining loss (for one batch) at step 67: 8.2754\nSeen so far: 4288 samples\nTraining loss (for one batch) at step 68: 8.3123\nSeen so far: 4352 samples\nTraining loss (for one batch) at step 69: 8.4867\nSeen so far: 4416 samples\nTraining loss (for one batch) at step 70: 8.4507\nSeen so far: 4480 samples\nTraining loss (for one batch) at step 71: 8.4101\nSeen so far: 4544 samples\nTraining loss (for one batch) at step 72: 8.3353\nSeen so far: 4608 samples\nTraining loss (for one batch) at step 73: 8.1866\nSeen so far: 4672 samples\nTraining loss (for one batch) at step 74: 8.4338\nSeen so far: 4736 samples\nTraining loss (for one batch) at step 75: 8.3941\nSeen so far: 4800 samples\nTraining loss (for one batch) at step 76: 8.2994\nSeen so far: 4864 samples\nTraining loss (for one batch) at step 77: 8.2918\nSeen so far: 4928 samples\nTraining loss (for one batch) at step 78: 8.2781\nSeen so far: 4992 samples\nTraining loss (for one batch) at step 79: 8.3472\nSeen so far: 5056 samples\nTraining loss (for one batch) at step 80: 8.5166\nSeen so far: 5120 samples\nTraining loss (for one batch) at step 81: 8.3320\nSeen so far: 5184 samples\nTraining loss (for one batch) at step 82: 8.3736\nSeen so far: 5248 samples\nTraining loss (for one batch) at step 83: 8.4363\nSeen so far: 5312 samples\nTraining loss (for one batch) at step 84: 8.3438\nSeen so far: 5376 samples\nTraining loss (for one batch) at step 85: 8.4157\nSeen so far: 5440 samples\nTraining loss (for one batch) at step 86: 8.4410\nSeen so far: 5504 samples\nTraining loss (for one batch) at step 87: 8.2167\nSeen so far: 5568 samples\nTraining loss (for one batch) at step 88: 8.3734\nSeen so far: 5632 samples\n","name":"stdout"},{"output_type":"stream","text":"Training loss (for one batch) at step 89: 8.2198\nSeen so far: 5696 samples\nTraining loss (for one batch) at step 90: 8.3300\nSeen so far: 5760 samples\nTraining loss (for one batch) at step 91: 8.5198\nSeen so far: 5824 samples\nTraining loss (for one batch) at step 92: 8.4234\nSeen so far: 5888 samples\nTraining loss (for one batch) at step 93: 8.3705\nSeen so far: 5952 samples\nTraining loss (for one batch) at step 94: 8.2913\nSeen so far: 6016 samples\nTraining loss (for one batch) at step 95: 8.4138\nSeen so far: 6080 samples\nTraining loss (for one batch) at step 96: 8.3103\nSeen so far: 6144 samples\nTraining loss (for one batch) at step 97: 8.3919\nSeen so far: 6208 samples\nTraining loss (for one batch) at step 98: 8.3477\nSeen so far: 6272 samples\nTraining loss (for one batch) at step 99: 8.2283\nSeen so far: 6336 samples\nTraining loss (for one batch) at step 100: 8.3095\nSeen so far: 6400 samples\nTraining loss (for one batch) at step 101: 8.3521\nSeen so far: 6464 samples\nTraining loss (for one batch) at step 102: 8.4651\nSeen so far: 6528 samples\nTraining loss (for one batch) at step 103: 8.4476\nSeen so far: 6592 samples\nTraining loss (for one batch) at step 104: 8.3869\nSeen so far: 6656 samples\nTraining loss (for one batch) at step 105: 8.2080\nSeen so far: 6720 samples\nTraining loss (for one batch) at step 106: 8.2625\nSeen so far: 6784 samples\nTraining loss (for one batch) at step 107: 8.4900\nSeen so far: 6848 samples\nTraining loss (for one batch) at step 108: 8.4392\nSeen so far: 6912 samples\nTraining loss (for one batch) at step 109: 8.2927\nSeen so far: 6976 samples\nTraining loss (for one batch) at step 110: 8.4613\nSeen so far: 7040 samples\nTraining loss (for one batch) at step 111: 8.4586\nSeen so far: 7104 samples\nTraining loss (for one batch) at step 112: 8.2494\nSeen so far: 7168 samples\nTraining loss (for one batch) at step 113: 8.2760\nSeen so far: 7232 samples\nTraining loss (for one batch) at step 114: 8.3731\nSeen so far: 7296 samples\nTraining loss (for one batch) at step 115: 8.4085\nSeen so far: 7360 samples\nTraining loss (for one batch) at step 116: 8.2231\nSeen so far: 7424 samples\nTraining loss (for one batch) at step 117: 8.3723\nSeen so far: 7488 samples\nTraining loss (for one batch) at step 118: 8.2887\nSeen so far: 7552 samples\nTraining loss (for one batch) at step 119: 8.2871\nSeen so far: 7616 samples\nTraining loss (for one batch) at step 120: 8.2793\nSeen so far: 7680 samples\nTraining loss (for one batch) at step 121: 8.3058\nSeen so far: 7744 samples\nTraining loss (for one batch) at step 122: 8.0814\nSeen so far: 7808 samples\nTraining loss (for one batch) at step 123: 8.4077\nSeen so far: 7872 samples\nTraining loss (for one batch) at step 124: 8.3586\nSeen so far: 7936 samples\nTraining loss (for one batch) at step 125: 8.5091\nSeen so far: 8000 samples\nTraining loss (for one batch) at step 126: 8.3601\nSeen so far: 8064 samples\nTraining loss (for one batch) at step 127: 8.4560\nSeen so far: 8128 samples\nTraining loss (for one batch) at step 128: 8.4513\nSeen so far: 8192 samples\nTraining loss (for one batch) at step 129: 8.4813\nSeen so far: 8256 samples\nTraining loss (for one batch) at step 130: 8.4107\nSeen so far: 8320 samples\nTraining loss (for one batch) at step 131: 8.5430\nSeen so far: 8384 samples\nTraining loss (for one batch) at step 132: 8.4105\nSeen so far: 8448 samples\nTraining loss (for one batch) at step 133: 8.4305\nSeen so far: 8512 samples\nTraining loss (for one batch) at step 134: 8.2719\nSeen so far: 8576 samples\nTraining loss (for one batch) at step 135: 8.4230\nSeen so far: 8640 samples\nTraining loss (for one batch) at step 136: 8.3961\nSeen so far: 8704 samples\nTraining loss (for one batch) at step 137: 8.2109\nSeen so far: 8768 samples\nTraining loss (for one batch) at step 138: 8.4519\nSeen so far: 8832 samples\nTraining loss (for one batch) at step 139: 8.4943\nSeen so far: 8896 samples\nTraining loss (for one batch) at step 140: 8.2955\nSeen so far: 8960 samples\nTraining loss (for one batch) at step 141: 8.4464\nSeen so far: 9024 samples\nTraining loss (for one batch) at step 142: 8.2920\nSeen so far: 9088 samples\nTraining loss (for one batch) at step 143: 8.3930\nSeen so far: 9152 samples\nTraining loss (for one batch) at step 144: 8.4303\nSeen so far: 9216 samples\nTraining loss (for one batch) at step 145: 8.4448\nSeen so far: 9280 samples\nTraining loss (for one batch) at step 146: 8.3279\nSeen so far: 9344 samples\nTraining loss (for one batch) at step 147: 8.2259\nSeen so far: 9408 samples\nTraining loss (for one batch) at step 148: 8.4211\nSeen so far: 9472 samples\nTraining loss (for one batch) at step 149: 8.2157\nSeen so far: 9536 samples\nTraining loss (for one batch) at step 150: 8.2920\nSeen so far: 9600 samples\nTraining loss (for one batch) at step 151: 8.3333\nSeen so far: 9664 samples\nTraining loss (for one batch) at step 152: 8.4235\nSeen so far: 9728 samples\nTraining loss (for one batch) at step 153: 8.2721\nSeen so far: 9792 samples\n\nEpoch: 4\nTraining loss (for one batch) at step 1: 8.3072\nSeen so far: 64 samples\nTraining loss (for one batch) at step 2: 8.3652\nSeen so far: 128 samples\nTraining loss (for one batch) at step 3: 8.3345\nSeen so far: 192 samples\nTraining loss (for one batch) at step 4: 8.1502\nSeen so far: 256 samples\nTraining loss (for one batch) at step 5: 8.1975\nSeen so far: 320 samples\nTraining loss (for one batch) at step 6: 8.5474\nSeen so far: 384 samples\nTraining loss (for one batch) at step 7: 8.2545\nSeen so far: 448 samples\nTraining loss (for one batch) at step 8: 8.3838\nSeen so far: 512 samples\nTraining loss (for one batch) at step 9: 8.1694\nSeen so far: 576 samples\nTraining loss (for one batch) at step 10: 8.3892\nSeen so far: 640 samples\nTraining loss (for one batch) at step 11: 8.2999\nSeen so far: 704 samples\nTraining loss (for one batch) at step 12: 8.3062\nSeen so far: 768 samples\nTraining loss (for one batch) at step 13: 8.3248\nSeen so far: 832 samples\nTraining loss (for one batch) at step 14: 8.4734\nSeen so far: 896 samples\nTraining loss (for one batch) at step 15: 8.4338\nSeen so far: 960 samples\nTraining loss (for one batch) at step 16: 8.3276\nSeen so far: 1024 samples\nTraining loss (for one batch) at step 17: 8.2431\nSeen so far: 1088 samples\nTraining loss (for one batch) at step 18: 8.1883\nSeen so far: 1152 samples\nTraining loss (for one batch) at step 19: 8.3062\nSeen so far: 1216 samples\nTraining loss (for one batch) at step 20: 8.2820\nSeen so far: 1280 samples\nTraining loss (for one batch) at step 21: 8.4553\nSeen so far: 1344 samples\nTraining loss (for one batch) at step 22: 8.4669\nSeen so far: 1408 samples\nTraining loss (for one batch) at step 23: 8.2990\nSeen so far: 1472 samples\nTraining loss (for one batch) at step 24: 8.4407\nSeen so far: 1536 samples\nTraining loss (for one batch) at step 25: 8.2420\nSeen so far: 1600 samples\nTraining loss (for one batch) at step 26: 8.4872\nSeen so far: 1664 samples\nTraining loss (for one batch) at step 27: 8.1368\nSeen so far: 1728 samples\nTraining loss (for one batch) at step 28: 8.2850\nSeen so far: 1792 samples\nTraining loss (for one batch) at step 29: 8.2497\nSeen so far: 1856 samples\nTraining loss (for one batch) at step 30: 8.3549\nSeen so far: 1920 samples\nTraining loss (for one batch) at step 31: 8.3740\nSeen so far: 1984 samples\nTraining loss (for one batch) at step 32: 8.3377\nSeen so far: 2048 samples\nTraining loss (for one batch) at step 33: 8.3351\nSeen so far: 2112 samples\nTraining loss (for one batch) at step 34: 8.2198\nSeen so far: 2176 samples\nTraining loss (for one batch) at step 35: 8.3603\nSeen so far: 2240 samples\nTraining loss (for one batch) at step 36: 8.1782\nSeen so far: 2304 samples\nTraining loss (for one batch) at step 37: 8.3052\nSeen so far: 2368 samples\nTraining loss (for one batch) at step 38: 8.2799\nSeen so far: 2432 samples\nTraining loss (for one batch) at step 39: 8.4937\nSeen so far: 2496 samples\nTraining loss (for one batch) at step 40: 8.3836\nSeen so far: 2560 samples\nTraining loss (for one batch) at step 41: 8.5265\nSeen so far: 2624 samples\nTraining loss (for one batch) at step 42: 8.3326\nSeen so far: 2688 samples\nTraining loss (for one batch) at step 43: 8.3299\nSeen so far: 2752 samples\nTraining loss (for one batch) at step 44: 8.3837\nSeen so far: 2816 samples\n","name":"stdout"},{"output_type":"stream","text":"Training loss (for one batch) at step 45: 8.2456\nSeen so far: 2880 samples\nTraining loss (for one batch) at step 46: 8.4196\nSeen so far: 2944 samples\nTraining loss (for one batch) at step 47: 8.1854\nSeen so far: 3008 samples\nTraining loss (for one batch) at step 48: 8.3470\nSeen so far: 3072 samples\nTraining loss (for one batch) at step 49: 8.1608\nSeen so far: 3136 samples\nTraining loss (for one batch) at step 50: 8.3810\nSeen so far: 3200 samples\nTraining loss (for one batch) at step 51: 8.3119\nSeen so far: 3264 samples\nTraining loss (for one batch) at step 52: 8.4359\nSeen so far: 3328 samples\nTraining loss (for one batch) at step 53: 8.2972\nSeen so far: 3392 samples\nTraining loss (for one batch) at step 54: 8.3620\nSeen so far: 3456 samples\nTraining loss (for one batch) at step 55: 8.3856\nSeen so far: 3520 samples\nTraining loss (for one batch) at step 56: 8.1973\nSeen so far: 3584 samples\nTraining loss (for one batch) at step 57: 8.3547\nSeen so far: 3648 samples\nTraining loss (for one batch) at step 58: 8.1506\nSeen so far: 3712 samples\nTraining loss (for one batch) at step 59: 8.3306\nSeen so far: 3776 samples\nTraining loss (for one batch) at step 60: 8.4197\nSeen so far: 3840 samples\nTraining loss (for one batch) at step 61: 8.3032\nSeen so far: 3904 samples\nTraining loss (for one batch) at step 62: 8.2262\nSeen so far: 3968 samples\nTraining loss (for one batch) at step 63: 8.3438\nSeen so far: 4032 samples\nTraining loss (for one batch) at step 64: 8.4470\nSeen so far: 4096 samples\nTraining loss (for one batch) at step 65: 8.2135\nSeen so far: 4160 samples\nTraining loss (for one batch) at step 66: 8.4068\nSeen so far: 4224 samples\nTraining loss (for one batch) at step 67: 8.3073\nSeen so far: 4288 samples\nTraining loss (for one batch) at step 68: 8.1981\nSeen so far: 4352 samples\nTraining loss (for one batch) at step 69: 8.1898\nSeen so far: 4416 samples\nTraining loss (for one batch) at step 70: 8.3488\nSeen so far: 4480 samples\nTraining loss (for one batch) at step 71: 8.3713\nSeen so far: 4544 samples\nTraining loss (for one batch) at step 72: 8.2008\nSeen so far: 4608 samples\nTraining loss (for one batch) at step 73: 8.4455\nSeen so far: 4672 samples\nTraining loss (for one batch) at step 74: 8.3291\nSeen so far: 4736 samples\nTraining loss (for one batch) at step 75: 8.3083\nSeen so far: 4800 samples\nTraining loss (for one batch) at step 76: 8.3564\nSeen so far: 4864 samples\nTraining loss (for one batch) at step 77: 8.3477\nSeen so far: 4928 samples\nTraining loss (for one batch) at step 78: 8.3757\nSeen so far: 4992 samples\nTraining loss (for one batch) at step 79: 8.4315\nSeen so far: 5056 samples\nTraining loss (for one batch) at step 80: 8.3198\nSeen so far: 5120 samples\nTraining loss (for one batch) at step 81: 8.3530\nSeen so far: 5184 samples\nTraining loss (for one batch) at step 82: 8.2097\nSeen so far: 5248 samples\nTraining loss (for one batch) at step 83: 8.3092\nSeen so far: 5312 samples\nTraining loss (for one batch) at step 84: 8.2398\nSeen so far: 5376 samples\nTraining loss (for one batch) at step 85: 8.4491\nSeen so far: 5440 samples\nTraining loss (for one batch) at step 86: 8.3416\nSeen so far: 5504 samples\nTraining loss (for one batch) at step 87: 8.3975\nSeen so far: 5568 samples\nTraining loss (for one batch) at step 88: 8.4616\nSeen so far: 5632 samples\nTraining loss (for one batch) at step 89: 8.1779\nSeen so far: 5696 samples\nTraining loss (for one batch) at step 90: 8.3945\nSeen so far: 5760 samples\nTraining loss (for one batch) at step 91: 8.2788\nSeen so far: 5824 samples\nTraining loss (for one batch) at step 92: 8.3577\nSeen so far: 5888 samples\nTraining loss (for one batch) at step 93: 8.2445\nSeen so far: 5952 samples\nTraining loss (for one batch) at step 94: 8.3991\nSeen so far: 6016 samples\nTraining loss (for one batch) at step 95: 8.3699\nSeen so far: 6080 samples\nTraining loss (for one batch) at step 96: 8.4097\nSeen so far: 6144 samples\nTraining loss (for one batch) at step 97: 8.3163\nSeen so far: 6208 samples\nTraining loss (for one batch) at step 98: 8.4004\nSeen so far: 6272 samples\nTraining loss (for one batch) at step 99: 8.3063\nSeen so far: 6336 samples\nTraining loss (for one batch) at step 100: 8.1849\nSeen so far: 6400 samples\nTraining loss (for one batch) at step 101: 8.3216\nSeen so far: 6464 samples\nTraining loss (for one batch) at step 102: 8.4484\nSeen so far: 6528 samples\nTraining loss (for one batch) at step 103: 8.2644\nSeen so far: 6592 samples\nTraining loss (for one batch) at step 104: 8.1483\nSeen so far: 6656 samples\nTraining loss (for one batch) at step 105: 8.2281\nSeen so far: 6720 samples\nTraining loss (for one batch) at step 106: 8.4054\nSeen so far: 6784 samples\nTraining loss (for one batch) at step 107: 8.2590\nSeen so far: 6848 samples\nTraining loss (for one batch) at step 108: 8.4763\nSeen so far: 6912 samples\nTraining loss (for one batch) at step 109: 8.3486\nSeen so far: 6976 samples\nTraining loss (for one batch) at step 110: 8.3194\nSeen so far: 7040 samples\nTraining loss (for one batch) at step 111: 8.1209\nSeen so far: 7104 samples\nTraining loss (for one batch) at step 112: 8.2828\nSeen so far: 7168 samples\nTraining loss (for one batch) at step 113: 8.2732\nSeen so far: 7232 samples\nTraining loss (for one batch) at step 114: 8.2509\nSeen so far: 7296 samples\nTraining loss (for one batch) at step 115: 8.3041\nSeen so far: 7360 samples\nTraining loss (for one batch) at step 116: 8.2072\nSeen so far: 7424 samples\nTraining loss (for one batch) at step 117: 8.3100\nSeen so far: 7488 samples\nTraining loss (for one batch) at step 118: 8.3696\nSeen so far: 7552 samples\nTraining loss (for one batch) at step 119: 8.4849\nSeen so far: 7616 samples\nTraining loss (for one batch) at step 120: 8.2146\nSeen so far: 7680 samples\nTraining loss (for one batch) at step 121: 8.4211\nSeen so far: 7744 samples\nTraining loss (for one batch) at step 122: 8.5095\nSeen so far: 7808 samples\nTraining loss (for one batch) at step 123: 8.2673\nSeen so far: 7872 samples\nTraining loss (for one batch) at step 124: 8.3451\nSeen so far: 7936 samples\nTraining loss (for one batch) at step 125: 8.4330\nSeen so far: 8000 samples\nTraining loss (for one batch) at step 126: 8.4411\nSeen so far: 8064 samples\nTraining loss (for one batch) at step 127: 8.3346\nSeen so far: 8128 samples\nTraining loss (for one batch) at step 128: 8.2319\nSeen so far: 8192 samples\nTraining loss (for one batch) at step 129: 8.4110\nSeen so far: 8256 samples\nTraining loss (for one batch) at step 130: 8.4955\nSeen so far: 8320 samples\nTraining loss (for one batch) at step 131: 8.4177\nSeen so far: 8384 samples\nTraining loss (for one batch) at step 132: 8.1893\nSeen so far: 8448 samples\nTraining loss (for one batch) at step 133: 8.3974\nSeen so far: 8512 samples\nTraining loss (for one batch) at step 134: 8.3525\nSeen so far: 8576 samples\nTraining loss (for one batch) at step 135: 8.5003\nSeen so far: 8640 samples\nTraining loss (for one batch) at step 136: 8.4399\nSeen so far: 8704 samples\nTraining loss (for one batch) at step 137: 8.2815\nSeen so far: 8768 samples\nTraining loss (for one batch) at step 138: 8.3053\nSeen so far: 8832 samples\nTraining loss (for one batch) at step 139: 8.4538\nSeen so far: 8896 samples\nTraining loss (for one batch) at step 140: 8.3629\nSeen so far: 8960 samples\nTraining loss (for one batch) at step 141: 8.3372\nSeen so far: 9024 samples\nTraining loss (for one batch) at step 142: 8.4025\nSeen so far: 9088 samples\nTraining loss (for one batch) at step 143: 8.3891\nSeen so far: 9152 samples\nTraining loss (for one batch) at step 144: 8.1711\nSeen so far: 9216 samples\nTraining loss (for one batch) at step 145: 8.2178\nSeen so far: 9280 samples\nTraining loss (for one batch) at step 146: 8.3143\nSeen so far: 9344 samples\nTraining loss (for one batch) at step 147: 8.3053\nSeen so far: 9408 samples\nTraining loss (for one batch) at step 148: 8.3608\nSeen so far: 9472 samples\nTraining loss (for one batch) at step 149: 8.1575\nSeen so far: 9536 samples\nTraining loss (for one batch) at step 150: 8.1865\nSeen so far: 9600 samples\nTraining loss (for one batch) at step 151: 8.2635\nSeen so far: 9664 samples\nTraining loss (for one batch) at step 152: 8.1916\nSeen so far: 9728 samples\nTraining loss (for one batch) at step 153: 8.2882\nSeen so far: 9792 samples\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = range(1, len(acc_loss) + 1) \nplt.plot(acc_loss[0], 'bo', label='Training acc') \nplt.plot(acc_loss[2], 'r', label='Validation acc') \nplt.legend() \nplt.figure() \nplt.plot(acc_loss[1], 'bo', label='Training loss')  \nplt.plot(acc_loss[3], 'r', label='Validation loss')\nplt.legend() \nplt.show()","execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model8.evaluate(X_test, y_test)\nprint(\"test loss, test acc:\", results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Increase data size"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_samples = 25000\ndf_partial = df_full.sample(n = n_samples, random_state=1)  \nnum_classes = len(df_partial.family_accession.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_encoder = LabelEncoder()\ny = label_encoder.fit_transform(df_partial.family_accession) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorized_sequences = [vectorized_sequence(sequence) for sequence in df_partial.sequence] \nmaxlen = 200\npadded_sequences = pad_sequences(vectorized_sequences , maxlen=maxlen, padding='post')\n\nX = padded_sequences\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model3 = Sequential([\n    Embedding(22, 8, input_length=maxlen),  \n    #Conv1D(64, 3, activation='relu'),\n    Flatten(),   \n    Dense(num_classes, activation ='softmax') \n]) \n\nmodel3.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n\nmodel3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 5\nbatch_size = 64\nacc_loss = train_val_model(model3,epochs,X_train,y_train,batch_size,X_val,y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = range(1, len(acc_loss) + 1) \nplt.plot(acc_loss[0], 'bo', label='Training acc') \nplt.plot(acc_loss[2], 'r', label='Validation acc') \nplt.legend() \nplt.figure() \nplt.plot(acc_loss[1], 'bo', label='Training loss')  \nplt.plot(acc_loss[3], 'r', label='Validation loss')\nplt.legend() \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model3.evaluate(X_test, y_test)\nprint(\"test loss, test acc:\", results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# Conclusion\n**^html link hidden here, fork notebook then click this cell to see it**\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Sed velit dignissim sodales ut eu sem integer vitae. Quam vulputate dignissim suspendisse in est ante. Arcu non odio euismod lacinia at. Etiam tempor orci eu lobortis. Egestas sed sed risus pretium quam vulputate dignissim suspendisse. Magna eget est lorem ipsum dolor sit amet consectetur adipiscing. Eget velit aliquet sagittis id consectetur purus ut faucibus. Egestas quis ipsum suspendisse ultrices gravida dictum fusce ut. Sed pulvinar proin gravida hendrerit. Dui faucibus in ornare quam viverra. Elit ullamcorper dignissim cras tincidunt lobortis feugiat vivamus. Enim diam vulputate ut pharetra sit amet aliquam. Nec feugiat in fermentum posuere urna nec tincidunt praesent. Elementum nisi quis eleifend quam adipiscing vitae proin sagittis. Malesuada proin libero nunc consequat interdum varius sit amet mattis. Urna neque viverra justo nec ultrices dui sapien. Adipiscing tristique risus nec feugiat in. Habitant morbi tristique senectus et netus et malesuada."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}